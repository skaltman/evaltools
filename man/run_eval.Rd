% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/run-eval.R
\name{run_eval}
\alias{run_eval}
\title{Run an evaluation}
\usage{
run_eval(
  samples_dir,
  solver_chat,
  system_prompt = NULL,
  tools_dir = "tools",
  tool_names = NULL,
  scorer_chat = ellmer::chat_anthropic(model = "claude-sonnet-4-5-20250929"),
  scorer_instructions = default_judge_instructions(),
  grade_levels = c("I", "C"),
  epochs = 1,
  name = "eval",
  dir = "logs",
  view = FALSE,
  ...
)
}
\arguments{
\item{samples_dir}{Path to directory with YAML sample files.}

\item{solver_chat}{A chat object, or a named list of chat objects to test
multiple models at once (e.g., \code{list(sonnet = chat1, opus = chat2)}).}

\item{system_prompt}{System prompt to prepend to all samples. Default NULL.}

\item{tools_dir}{Path to directory with tool R files to source. Default "tools".
Set to NULL if tools are already loaded (e.g., from a package).}

\item{tool_names}{Tool names to check in scorer. Default NULL auto-detects from
YAML files.}

\item{scorer_chat}{Chat object for LLM judge. Default uses Claude Sonnet 4.5.}

\item{scorer_instructions}{Custom instructions for the judge. Default uses
Correct/Incorrect grading.}

\item{grade_levels}{Valid grades. Default \code{c("I", "C")}.}

\item{epochs}{Number of evaluation runs. Default 1.}

\item{name}{Task name. Default "eval".}

\item{dir}{Logging directory. Default "logs".}

\item{view}{Open interactive viewer after running? Default FALSE. If you get
port conflicts, use \code{vitals::vitals_view(port = custom_port)} instead.}

\item{...}{Additional arguments for \code{create_solver()} (e.g., prompt_field,
setup_field, sleep_time).}
}
\value{
A tibble with one row per sample per model, containing:
\itemize{
\item \code{model}: Which model produced the result
\item \code{id}: Sample ID from YAML file
\item \code{epoch}: Evaluation epoch number
\item \code{score}: Grade ("C" for Correct, "I" for Incorrect, etc.)
\item \code{metadata}: List column with solver and scorer details
}
Task objects are stored in \code{attr(results, "tasks")} for advanced use.
}
\description{
Run an evaluation and get results as a tibble. Point it at your samples
directory and provide one or more language model chat objects. Results
include a model column, scores for each sample, and detailed metadata.
}
\examples{
\dontrun{
# Single model evaluation
results <- run_eval(
  samples_dir = "samples/",
  solver_chat = ellmer::chat_anthropic(model = "claude-sonnet-4-5-20250929")
)

# Calculate accuracy
results |>
  dplyr::summarize(accuracy = mean(score == "C"))

# Multiple models at once
results <- run_eval(
  samples_dir = "samples/",
  solver_chat = list(
    sonnet = ellmer::chat_anthropic(model = "claude-sonnet-4-5-20250929"),
    opus = ellmer::chat_anthropic(model = "claude-opus-4-20250514")
  )
)

# Compare accuracy by model
results |>
  dplyr::group_by(model) |>
  dplyr::summarize(accuracy = mean(score == "C"))

# Access original Task objects if needed
tasks <- attr(results, "tasks")
tasks$sonnet$accuracy()

# If you need to view results and have port conflicts:
results <- run_eval(
  samples_dir = "samples/",
  solver_chat = chat_anthropic(model = "claude-sonnet-4-5-20250929"),
  view = FALSE  # Disable automatic viewer
)
# Then manually view with custom port
vitals::vitals_view(port = 8888)
}
}
